# Modelo-de-previs-o-para-Hackathon
Objetivo do Projeto
Este projeto tem como objetivo desenvolver um modelo de previs√£o de vendas (forecast) para o varejo. A solu√ß√£o prediz a quantidade de vendas semanal por PDV e SKU para as cinco primeiras semanas de janeiro de 2023, utilizando o hist√≥rico de vendas de 2022.

üí° Metodologia e Estrat√©gia
Nossa abordagem foi guiada por um pipeline robusto de Ci√™ncia de Dados focado em efici√™ncia e escalabilidade, que se mostrou crucial devido ao grande volume de dados.

Amostragem Segura e Uni√£o dos Dados: Para superar os desafios de mem√≥ria RAM devido ao limite de processamento dispon√≠vel, a estrat√©gia foi trabalhar todo o arquivo de 6.560.698 linhas da base de transa√ß√µes e uma amostra de mesmo tamanho para os produtos que possui um arquivo maior 195 milh√µes de linhas. Os dados de PDV, Transa√ß√µes e de produtos foram filtrados para corresponder a essa amostra, garantindo uma jun√ß√£o eficiente.

Utilizei ambiente de Linguagem R no google colab, visto que o computador n√£o possuia capacidade, com isso possu√≠a apenas 12GB de RAM para o processamento.
Nos primeiro dias, foi b√°sicamente para encontra um pacote e biblioteca que pudess ler o parquet, como nunca tinha utilizado um arquivo t√£o grande, pocurei alternativas
para a obten√ß√£o de Dataframe compat√≠vel com a mem√≥ria do ambiente virtual.

Todas quase todas as tentativas foram fracassadas, at√© que encontrei uma possibilidade utilizando o pacote "duckdb", sendo esse crucial para a manipula√ß√£o, pois com ele pude utilizar SELECT*FROM, para entrar no ds_produtos e coletar uma amostra de 6.560.698 linhas, e poder seguir com a modelagem.

An√°lise Explorat√≥ria de Dados (EDA):

Foram criados gr√°ficos para analisar o comportamento dos PDVs, identificando que a maioria das lojas √© do tipo Off Premise.
<img width="840" height="840" alt="download" src="https://github.com/user-attachments/assets/f6e66552-3b50-42f6-bdf5-4591b67ce0fa" />

Gr√°ficos de s√©rie temporal mostraram uma forte sazonalidade, com quedas de vendas nos finais de semana e feriados, e picos em datas de promo√ß√£o.
<img width="840" height="840" alt="download (14)" src="https://github.com/user-attachments/assets/aaed9d7c-4bec-4a24-86a0-48be996d4ff3" />
<img width="840" height="840" alt="download (12)" src="https://github.com/user-attachments/assets/3457de00-5d9d-4d53-a753-1945cce18ace" />
<img width="840" height="840" alt="download (13)" src="https://github.com/user-attachments/assets/b16cf2b8-33d0-4f7c-aeed-1c2dacff7c6c" />

Durante as visualiza√ß√µes foi verificado que as promo√ß√µes n√£o tiveram um desempenho esperado ent√£o surgiu a hipotese: 
#hipotese de que a queda nas venda em promo√ß√£o seja pelo fato de que as mesmas
#tenha ocorrido em finais de semanas ou feriados

      promocoes_2022 <- as.Date(c(
        "2022-03-25",
        "2022-07-10",
        "2022-11-25"
      ))
      df_eventos <- vendas_diarias |>
        mutate(is_holiday = ifelse(transaction_date %in% feriados_2022, "Sim", "N√£o"),
               is_promotion = ifelse(transaction_date %in% promocoes_2022, "Sim", "N√£o"),
               is_weekend = ifelse(dia_da_semana %in% c(6, 7), "Sim", "N√£o"))
      analise_eventos <- df_eventos |>
        group_by(is_weekend, is_holiday, is_promotion) |>
        summarise(
          media_vendas = mean(total_quantity, na.rm = TRUE),
          n_dias = n(),
          .groups = "drop"
        )
      
        df_analise_eventos <- analise_eventos |>
        collect()
      
        print(df_analise_eventos, n = Inf)

   A tibble: 6 √ó 5
  is_weekend is_holiday is_promotion media_vendas n_dias
  <chr>      <chr>      <chr>               <dbl>  <int>
1 N√£o        N√£o        N√£o                90162.    249
2 Sim        N√£o        N√£o               295654.    101
3 N√£o        Sim        N√£o                99853.      9
4 Sim        N√£o        Sim                 4391.      1
5 N√£o        N√£o        Sim                43426.      2
6 Sim        Sim        N√£o                 3445.      3

O que foi mostrado que durante a avalia√ß√£o houve uma queda nas vendas da promo√ß√£o e uma delas foi em um final de semana.

Essa an√°lise foi a base para a cria√ß√£o de features relevantes.

Engenharia de Vari√°veis (Feature Engineering):

Os dados foram agregados a um n√≠vel de granularidade semanal por PDV e SKU, conforme a necessidade do desafio.
        produtos_unique <- df_produtos_amostra |>
          group_by(produto) |>
          summarise(
            categoria = first(na.omit(categoria)),
            descricao = first(na.omit(descricao)),
            tipos = first(na.omit(tipos)),
            label = first(na.omit(label)),
            subcategoria = first(na.omit(subcategoria)),
            marca = first(na.omit(marca)),
            fabricante = first(na.omit(fabricante)),
            .groups = "drop"
          )

                  df_vendas_enriquecida <- df_vendas_amostra |>
                  left_join(produtos_unique, by = c("internal_product_id" = "produto"))
                        df_vendas_final <- df_vendas_enriquecida |>
                        left_join(df_pdv_amostra, by = c("internal_store_id" = "pdv"))

Foram criadas vari√°veis de lag (vendas passadas) e media_movel_3 (m√©dia m√≥vel de 3 semanas) para capturar a tend√™ncia e o comportamento da s√©rie temporal.

Foram adicionadas features categ√≥ricas (como categoria_pdv e premise) para o modelo.

Utilizamos filtragens para pode encontra valores √∫nicos, e dar mais leveza para a modelagem.

                    pdvs_amostra <- unique(df_vendas_amostra$internal_store_id)
                    ds_pdv <- open_dataset("part-00000-tid-2779033056155408584-f6316110-4c9a-4061-ae48-69b77c7c8c36-4-1-c000.snappy.parquet")
                    df_pdv_amostra <- ds_pdv |>
                      filter(pdv %in% pdvs_amostra) |>
                      collect()|>
                      dplyr::sample_frac()

Modelagem Preditiva:

A escolha do modelo foi o XGBoost, um algoritmo de Machine Learning robusto para dados estruturados, que tem um excelente desempenho em problemas de previs√£o.

O modelo foi treinado em uma amostra dos dados de 2022, utilizando nrounds=100 e uma divis√£o de treino e teste para valida√ß√£o.

üìä An√°lise Visual dos Dados
Aqui est√£o alguns dos gr√°ficos que criamos para entender melhor os dados:

Distribui√ß√£o de PDVs por Tipo de Premise

An√°lise: A maioria dos PDVs √© do tipo 'Off Premise', indicando um comportamento de vendas dominante nesse segmento.

Quantidade por Categoria PDV (Ordem Decrescente)

An√°lise: Este gr√°fico de barras revela a distribui√ß√£o da quantidade de PDVs por categoria. Uma an√°lise valiosa para entender as categorias de PDV com maior presen√ßa.

Vendas Di√°rias com Destaque para Finais de Semana

An√°lise: O gr√°fico de s√©rie temporal mostra que as vendas t√™m uma forte sazonalidade semanal, com quedas nos finais de semana. Esta √© uma feature crucial para o modelo.

Vendas Di√°rias com Destaque para Feriados

An√°lise: As linhas vermelhas destacam os feriados, mostrando uma queda acentuada nas vendas nesses dias.

Vendas Di√°rias com Destaque para Promo√ß√µes

An√°lise: As linhas roxas indicam os dias de promo√ß√£o, que geralmente levam a picos de vendas.

Vendas por Premise (On/Off)

An√°lise: Este gr√°fico mostra que o volume de vendas para a premissa 'Off Premise' √© significativamente maior do que para 'On Premise'.

Quantidade Vendida por Categoria

An√°lise: Este gr√°fico de barras horizontais destaca as categorias de produtos que mais vendem, em termos de quantidade.

Lucro Bruto por Categoria

An√°lise: O lucro bruto por categoria fornece insights sobre a rentabilidade de cada segmento de produto.

Ticket M√©dio por Premissa

An√°lise: Este gr√°fico compara o valor m√©dio das transa√ß√µes entre as premissas, mostrando que 'Off Premise' tem um ticket m√©dio mais alto.

Desconto M√©dio por Categoria de PDV

An√°lise: A visualiza√ß√£o do desconto m√©dio por categoria de PDV pode ajudar a identificar padr√µes de precifica√ß√£o e seu impacto nas vendas.

Evolu√ß√£o Mensal das Vendas por Premissa

An√°lise: Este gr√°fico de linhas demonstra a tend√™ncia das vendas ao longo dos meses para cada tipo de premissa, evidenciando o padr√£o de sazonalidade anual.

üìà Resultados e Avalia√ß√£o
A performance do modelo de XGBoost foi avaliada com as seguintes m√©tricas, que indicam um alto poder preditivo na base de valida√ß√£o:

RMSE: 5.83 (Erro m√©dio nas previs√µes)

R¬≤: 0.827 (Poder de explica√ß√£o do modelo, que √© de 82.7%)

MAE: 1.027 (Erro absoluto m√©dio)


üöÄ Como Usar e Pr√©-requisitos

Pr√©-requisitos: Instale as bibliotecas R necess√°rias em seu ambiente (idealmente no Google Colab ou RStudio) usando o seguinte comando:
          install.packages("arrow") #instalando biblioteca de leitura
          install.packages("caret")
          install.packages("xgboost")
          install.packages("tidyverse") #instalando biblioteca de manipula√ß√£o
          install.packages ("forecast")
          install.packages("duckdb")

Carregando os pacotes para a sess√£o:
        library(arrow)
        library(dplyr)
        library(lubridate)
        library(duckdb)
        library(zoo)
        library(tidyverse)
        library(ggplot2)
        library(shiny)
        library(forecast)
        library(forcats)
        library(scales)
        library(xgboost)
        library(caret)

Estrutura de Arquivos: Certifique-se de que os arquivos .parquet estejam prontos para utiliza√ß√£o.

Execu√ß√£o: O c√≥digo est√° contido em um √∫nico script, projetado para ser executado em ordem sequencial. Ele realizar√° a amostragem, a uni√£o dos dados, a agrega√ß√£o e a modelagem, gerando o arquivo de submiss√£o final.                                                                                                                              

üßë‚Äçüíª Equipe
Izan Cassio Nascimento Pereira
Lincoln 
Pedro
